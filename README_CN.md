![logo](logo.png)

**[English](README.md) | [ç®€ä½“ä¸­æ–‡](README_CN.md)**

# ğŸ¤– ç®€ä»‹

Intern-S1 æŠ€æœ¯æŠ¥å‘Šä¸­æåˆ°è¯¥æ¨¡å‹åŸºäº Qwen3 å’Œ InternVL-Vit åšçš„ç»§ç»­é¢„è®­ç»ƒï¼Œä½†è®ºæ–‡ä¸­æš‚æ—¶æ²¡æœ‰æ›´æ–°å…³äº Intern-S1-Pro çš„è¯¦ç»†æŠ€æœ¯ä»‹ç»ï¼Œå› æ­¤æœ¬é¡¹ç›®æ¢ç´¢ Qwen3-VL-235B-A22B-Instruct å’Œ Intern-S1-Pro ä¸¤ä¸ªå¤šæ¨¡æ€ MoE æ¨¡å‹çš„ config é…ç½®ã€modeling ä»£ç ç­‰ï¼Œå¹¶è¿›è¡Œå¯¹æ¯”åˆ†æã€‚

**TL;DR**: Intern-S1-Pro é€šè¿‡å°† Qwen3-235B-A22B çš„ MoE ä¸“å®¶æ•°ä» 128 æ‰©å±•åˆ° 512ï¼ˆ4å€ï¼‰ï¼Œåœ¨ç›¸åŒæ¶æ„ä¸‹å®ç°äº† 235B â†’ 1T çš„å‚æ•° Scaling Upï¼Œä¿æŒäº†ç›¸ä¼¼çš„æ¨ç†æ•ˆç‡ï¼ˆæ¿€æ´»å‚æ•°å‡ä¸º~22Bï¼‰ã€‚å…·ä½“æ‰©å±•ä¸“å®¶æ˜¯å¦ä»¥åŠå¦‚ä½•å¤ç”¨ Qwen3-235B-A22B ä¸“å®¶æƒé‡ï¼Œæš‚æ—¶æœªçŸ¥ã€‚

## ğŸ“¦ é¡¹ç›®ç»“æ„

```
intern-s1-pro-vs-qwen3-235b-a22b/
â”œâ”€â”€ checkpoints/
â”‚   â”œâ”€â”€ Intern-S1-Pro/              # ä¸å«æƒé‡
â”‚   â”‚   â”œâ”€â”€ config.json             # æ¨¡å‹é…ç½®
â”‚   â”‚   â”œâ”€â”€ modeling_*.py           # æ¨¡å‹å®ç°
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ Qwen3-VL-235B-A22B-Instruct/  # ä¸å«æƒé‡
â”‚       â”œâ”€â”€ config.json
â”‚       â”œâ”€â”€ modeling_*.py
â”‚       â””â”€â”€ ...
â”œâ”€â”€ logo.png
â”œâ”€â”€ README.md                       # æœ¬æ–‡ä»¶ (English)
â””â”€â”€ README_CN.md                    # ä¸­æ–‡ç‰ˆ
```


## ğŸ’¡ æ¨¡å‹å¯¹æ¯”æ€»è§ˆ

| ç‰¹æ€§ | Intern-S1-Pro | Qwen3-VL-235B-A22B |
|------|--------------|-------------------|
| **æ€»å‚æ•°é‡** | **~920B (â‰ˆ1T)** | **~235B** |
| **æ¿€æ´»å‚æ•°** | 22B | 22B |
| **MoE ä¸“å®¶æ•°** | **512** | **128** |
| **ä¸“å®¶æ¿€æ´»æ•°** | 8 | 8 |
| **éšè—å±‚æ•°** | 94 | 94 |
| **éšè—ç»´åº¦** | 4096 | 4096 |
| **æ³¨æ„åŠ›å¤´** | 64 | 64 |
| **KV å¤´æ•°** | 4 (GQA) | 4 (GQA) |
| **ä¸Šä¸‹æ–‡é•¿åº¦** | 262K | 262K |

## âš™ï¸ Config é…ç½®è¯¦ç»†å¯¹æ¯”

### æ–‡æœ¬æ¨¡å‹é…ç½® (text_config)

| é…ç½®é¡¹ | Intern-S1-Pro | Qwen3-VL-235B | è¯´æ˜ |
|--------|--------------|---------------|------|
| **åŸºç¡€æ¶æ„** |  |  |  |
| model_type | `interns1_pro_text` | `qwen3_vl_moe_text` | æ¨¡å‹ç±»å‹æ ‡è¯† |
| hidden_size | 4096 | 4096 | éšè—å±‚ç»´åº¦ |
| num_hidden_layers | 94 | 94 | Transformer å±‚æ•° |
| intermediate_size | 12288 | 12288 | FFN ä¸­é—´å±‚ç»´åº¦ |
| **æ³¨æ„åŠ›æœºåˆ¶** |  |  |  |
| num_attention_heads | 64 | 64 | æ³¨æ„åŠ›å¤´æ•°é‡ |
| num_key_value_heads | 4 | 4 | KV å¤´æ•°é‡ï¼ˆGQAï¼‰ |
| head_dim | 128 | 128 | æ¯ä¸ªæ³¨æ„åŠ›å¤´ç»´åº¦ |
| attention_dropout | 0.0 | 0.0 | æ³¨æ„åŠ›dropout |
| attention_bias | false | false | æ³¨æ„åŠ›æ˜¯å¦ä½¿ç”¨åç½® |
| **MoE é…ç½®** â­ |  |  |  |
| **num_experts** | **512** | **128** | ğŸ”¥ ä¸“å®¶æ€»æ•°ï¼ˆ4å€å·®å¼‚ï¼‰ |
| num_experts_per_tok | 8 | 8 | æ¯ä¸ªtokenæ¿€æ´»çš„ä¸“å®¶æ•° |
| moe_intermediate_size | 1536 | 1536 | æ¯ä¸ªä¸“å®¶çš„ä¸­é—´ç»´åº¦ |
| **router_n_groups** | **8** | âŒ | ğŸ”¥ è·¯ç”±åˆ†ç»„æ•°ï¼ˆInternç‹¬æœ‰ï¼‰ |
| norm_topk_prob | true | true | æ˜¯å¦å½’ä¸€åŒ–top-kæ¦‚ç‡ |
| decoder_sparse_step | 1 | 1 | è§£ç å™¨ç¨€ç–æ­¥é•¿ |
| **è¯è¡¨ä¸Token** |  |  |  |
| vocab_size | 155,008 | 151,936 | è¯è¡¨å¤§å° |
| bos_token_id | 151643 | 151643 | èµ·å§‹token ID |
| eos_token_id | 151645 | 151645 | ç»“æŸtoken ID |
| **ä½ç½®ç¼–ç ** |  |  |  |
| rope_theta | 5,000,000 | 5,000,000 | RoPE åŸºç¡€é¢‘ç‡ |
| rope_type | `default` | `default` | RoPE ç±»å‹ |
| **fope_init_factor** | **0.5** | âŒ | ğŸ”¥ FoPE åˆå§‹åŒ–å› å­ï¼ˆInternç‹¬æœ‰ï¼‰ |
| **fope_sep_head** | **true** | âŒ | ğŸ”¥ FoPE åˆ†ç¦»å¤´ï¼ˆInternç‹¬æœ‰ï¼‰ |
| **mrope_interleaved** | âŒ | **true** | ğŸ”¥ äº¤é”™MRoPEï¼ˆQwenç‹¬æœ‰ï¼‰ |
| **mrope_section** | âŒ | **[24, 20, 20]** | ğŸ”¥ MRoPEåˆ†æ®µï¼ˆQwenç‹¬æœ‰ï¼‰ |
| max_position_embeddings | 262,144 | 262,144 | æœ€å¤§ä½ç½®ç¼–ç é•¿åº¦ï¼ˆ256Kï¼‰ |
| **å½’ä¸€åŒ–** |  |  |  |
| rms_norm_eps | 1e-06 | 1e-06 | RMSNorm epsilon |
| **å…¶ä»–** |  |  |  |
| hidden_act | `silu` | `silu` | æ¿€æ´»å‡½æ•° |
| initializer_range | 0.02 | 0.02 | å‚æ•°åˆå§‹åŒ–èŒƒå›´ |
| mlp_only_layers | [] | [] | ä»…MLPçš„å±‚ |
| use_cache | true | true | æ˜¯å¦ä½¿ç”¨KVç¼“å­˜ |
| dtype | bfloat16 | bfloat16 | æ•°æ®ç±»å‹ |

### è§†è§‰æ¨¡å‹é…ç½® (vision_config)

| é…ç½®é¡¹ | Intern-S1-Pro | Qwen3-VL-235B | è¯´æ˜ |
|--------|--------------|---------------|------|
| **åŸºç¡€æ¶æ„** |  |  |  |
| model_type | `interns1_pro_vision` | `qwen3_vl_moe` | è§†è§‰æ¨¡å‹ç±»å‹ |
| **depth** | **24** | **27** | ViT å±‚æ•° |
| **hidden_size** | **1024** | **1152** | éšè—å±‚ç»´åº¦ |
| **intermediate_size** | **4096** | **4304** | FFN ä¸­é—´ç»´åº¦ |
| num_heads | 16 | 16 | æ³¨æ„åŠ›å¤´æ•° |
| **è¡¥ä¸é…ç½®** |  |  |  |
| patch_size | 16 | 16 | å›¾åƒè¡¥ä¸å¤§å° |
| temporal_patch_size | 2 | 2 | æ—¶é—´è¡¥ä¸å¤§å°ï¼ˆè§†é¢‘ï¼‰ |
| spatial_merge_size | 2 | 2 | ç©ºé—´åˆå¹¶å°ºå¯¸ |
| in_channels | 3 | 3 | è¾“å…¥é€šé“æ•°ï¼ˆRGBï¼‰ |
| **ç‰¹å¾èåˆ** |  |  |  |
| **deepstack_visual_indexes** | âŒ | **[8, 16, 24]** | ğŸ”¥ DeepStackå±‚ç´¢å¼•ï¼ˆQwenç‹¬æœ‰ï¼‰ |
| **è¾“å‡ºé…ç½®** |  |  |  |
| out_hidden_size | 4096 | 4096 | è¾“å‡ºéšè—ç»´åº¦ï¼ˆå¯¹é½æ–‡æœ¬ï¼‰ |
| num_position_embeddings | 2304 | 2304 | ä½ç½®ç¼–ç æ•°é‡ |
| **å…¶ä»–** |  |  |  |
| hidden_act | `gelu_pytorch_tanh` | `gelu_pytorch_tanh` | æ¿€æ´»å‡½æ•° |
| initializer_range | 0.02 | 0.02 | å‚æ•°åˆå§‹åŒ–èŒƒå›´ |

### é‡åŒ–é…ç½® (quantization_config)

| é…ç½®é¡¹ | Intern-S1-Pro | Qwen3-VL-235B | è¯´æ˜ |
|--------|--------------|---------------|------|
| **é‡åŒ–æ–¹æ³•** | **FP8** | âŒ | Intern-S1-Pro ä½¿ç”¨ FP8 é‡åŒ– |
| quant_method | `fp8` | - | é‡åŒ–æ–¹æ³• |
| fmt | `e4m3` | - | FP8 æ ¼å¼ï¼ˆ4ä½æŒ‡æ•°ï¼Œ3ä½å°¾æ•°ï¼‰ |
| scale_fmt | `ue8m0` | - | ç¼©æ”¾å› å­æ ¼å¼ |
| weight_block_size | [128, 128] | - | æƒé‡å—å¤§å° |
| activation_scheme | `dynamic` | - | æ¿€æ´»é‡åŒ–æ–¹æ¡ˆ |
| modules_to_not_convert | 698ä¸ªæ¨¡å— | - | ä¸è½¬æ¢çš„æ¨¡å—åˆ—è¡¨ |

### ç‰¹æ®Š Token é…ç½®

| Token | Intern-S1-Pro | Qwen3-VL-235B | è¯´æ˜ |
|-------|--------------|---------------|------|
| image_token_id | 151655 | 151655 | å›¾åƒå ä½ç¬¦ |
| video_token_id | 151656 | 151656 | è§†é¢‘å ä½ç¬¦ |
| vision_start_token_id | 151652 | 151652 | è§†è§‰å¼€å§‹æ ‡è®° |
| vision_end_token_id | 151653 | 151653 | è§†è§‰ç»“æŸæ ‡è®° |

### ğŸ” å…³é”®é…ç½®å·®å¼‚æ€»ç»“

#### 1ï¸âƒ£ **MoE ä¸“å®¶æ•°é‡** (æœ€æ ¸å¿ƒå·®å¼‚)
- **Intern-S1-Pro**: 512 ä¸“å®¶ + 8 ç»„è·¯ç”±ç®¡ç†
- **Qwen3-VL**: 128 ä¸“å®¶ï¼Œæ— åˆ†ç»„
- **å½±å“**: ç›´æ¥å¯¼è‡´ ~4 å€å‚æ•°é‡å·®å¼‚

#### 2ï¸âƒ£ **ä½ç½®ç¼–ç ç­–ç•¥**
- **Intern-S1-Pro**: FoPE (Fourier Position Encoding)
  - ä¸“ä¸ºç§‘å­¦ä¿¡å·å’Œæ—¶é—´åºåˆ—ä¼˜åŒ–
  - `fope_init_factor=0.5`, `fope_sep_head=true`
- **Qwen3-VL**: Interleaved MRoPE
  - å¤šç»´åº¦äº¤é”™ç¼–ç ï¼ˆæ—¶é—´ã€å®½åº¦ã€é«˜åº¦ï¼‰
  - `mrope_section=[24, 20, 20]` ä¸‰ç»´åˆ†é…

#### 3ï¸âƒ£ **è§†è§‰ç¼–ç å™¨**
- **Intern-S1-Pro**: 24 å±‚ï¼Œ1024 ç»´ï¼Œæ›´è½»é‡
- **Qwen3-VL**: 27 å±‚ï¼Œ1152 ç»´ï¼Œæ”¯æŒ DeepStack å¤šå±‚ç‰¹å¾èåˆ

#### 4ï¸âƒ£ **é‡åŒ–æ”¯æŒ**
- **Intern-S1-Pro**: å†…ç½® FP8 é‡åŒ–ï¼ˆe4m3æ ¼å¼ï¼‰
  - 698 ä¸ªå…³é”®æ¨¡å—ä¿æŒé«˜ç²¾åº¦
- **Qwen3-VL**: æœªå†…ç½®é‡åŒ–é…ç½®

#### 5ï¸âƒ£ **è¯è¡¨å¤§å°**
- **Intern-S1-Pro**: 155,008 tokens
  - é¢å¤–åŒ…å«ä¸“ä¸šé¢†åŸŸ tokenizerï¼ˆPROTã€SMILESã€XNAï¼‰
- **Qwen3-VL**: 151,936 tokens


## ğŸ“Š è¯¦ç»†å‚æ•°é‡åˆ†æ

### Intern-S1-Pro (~1T å‚æ•°)

```
æ€»å‚æ•°é‡: 920B
â”œâ”€ æ–‡æœ¬æ¨¡å‹:      916.56B (99.97%)
â”‚  â”œâ”€ Embedding:    0.63B
â”‚  â”œâ”€ 94 Layers:  915.29B
â”‚  â”‚  â”œâ”€ Attention per layer:    0.071B
â”‚  â”‚  â”œâ”€ MoE Experts (512ä¸ª):    9.66B  â¬…ï¸ æ ¸å¿ƒå·®å¼‚
â”‚  â”‚  â””â”€ Router per layer:       2.10M
â”‚  â””â”€ LM Head:      0.63B
â””â”€ è§†è§‰æ¨¡å‹:        0.31B (0.03%)
   â”œâ”€ æ·±åº¦: 24 å±‚
   â”œâ”€ éšè—ç»´åº¦: 1024
   â””â”€ ä¸­é—´ç»´åº¦: 4096

æ¿€æ´»å‚æ•°: 22.36B (ä»… 2.4%)
```

### Qwen3-VL-235B-A22B (~235B å‚æ•°)

```
æ€»å‚æ•°é‡: 235.51B
â”œâ”€ æ–‡æœ¬æ¨¡å‹:      235.09B (99.82%)
â”‚  â”œâ”€ Embedding:    0.62B
â”‚  â”œâ”€ 94 Layers:  233.85B
â”‚  â”‚  â”œâ”€ Attention per layer:    0.071B
â”‚  â”‚  â”œâ”€ MoE Experts (128ä¸ª):    2.42B
â”‚  â”‚  â””â”€ Router per layer:       0.52M
â”‚  â””â”€ LM Head:      0.62B
â””â”€ è§†è§‰æ¨¡å‹:        0.42B (0.18%)
   â”œâ”€ æ·±åº¦: 27 å±‚
   â”œâ”€ éšè—ç»´åº¦: 1152
   â””â”€ ä¸­é—´ç»´åº¦: 4304

æ¿€æ´»å‚æ•°: 22.19B (9.4%)
```

### ğŸ” ä¸ºä»€ä¹ˆ Intern-S1-Pro æ˜¯ 1T å‚æ•°ï¼Ÿ

**å…³é”®åŸå› ï¼šMoE ä¸“å®¶æ•°é‡å·®å¼‚**

- **Intern-S1-Pro**: 512 ä¸ªä¸“å®¶
- **Qwen3-VL**: 128 ä¸ªä¸“å®¶
- **å·®è·**: **4 å€**

**æ¯å±‚ MoE å‚æ•°é‡**
- Intern-S1-Pro æ¯å±‚: 9.66B (512 ä¸“å®¶)
- Qwen3-VL æ¯å±‚: 2.42B (128 ä¸“å®¶)
- 94 å±‚ç´¯ç§¯ï¼š**681B vs 170B** çš„å·®è·

**ä½†å®é™…æ¨ç†æ•ˆç‡ç›¸ä¼¼**
- ä¸¤ä¸ªæ¨¡å‹éƒ½åªæ¿€æ´» **8 ä¸ªä¸“å®¶**
- æ¿€æ´»å‚æ•°éƒ½çº¦ä¸º **22B**
- æ¨ç†æ˜¾å­˜å’Œè®¡ç®—æˆæœ¬æ¥è¿‘



## âœ¨ æ¨¡å‹ç‰¹ç‚¹å¯¹æ¯”

### Intern-S1-Pro ç‰¹è‰²åŠŸèƒ½

#### ğŸ”¬ ç§‘å­¦æ¨ç†ä¸“ç²¾
- **AI4Science ä¼˜åŒ–**: åŒ–å­¦ã€ææ–™ã€ç”Ÿå‘½ç§‘å­¦ã€åœ°çƒç§‘å­¦
- **ç‰©ç†ä¿¡å·å»ºæ¨¡**: æ”¯æŒé•¿æ—¶é—´åºåˆ—ï¼ˆ10^0 ~ 10^6 ä¸ªç‚¹ï¼‰
- **ä¸“ä¸š Tokenizer**:
  - `tokenizer_PROT.model` - è›‹ç™½è´¨åºåˆ—
  - `tokenizer_SMILES.model` - åŒ–å­¦åˆ†å­å¼
  - `tokenizer_XNA.model` - æ ¸é…¸åºåˆ—

#### ğŸ§  Thinking Mode
- é»˜è®¤å¯ç”¨æ€è€ƒæ¨¡å¼å¢å¼ºæ¨ç†
- å¯é€šè¿‡ `enable_thinking=False` å…³é—­

#### âš™ï¸ æŠ€æœ¯åˆ›æ–°
- **FoPE (Fourier Position Encoding)**: æ›´å¥½çš„ä½ç½®ç¼–ç 
- **STE Routing**: å¯†é›†æ¢¯åº¦è·¯ç”±è®­ç»ƒ
- **Grouped Routing**: 8 ç»„ä¸“å®¶ç®¡ç†ï¼ˆrouter_n_groups=8ï¼‰
- **FP8 é‡åŒ–**: é™ä½æ˜¾å­˜å ç”¨

### Qwen3-VL ç‰¹è‰²åŠŸèƒ½

#### ğŸŒ é€šç”¨è§†è§‰ç†è§£
- **Visual Agent**: æ“ä½œ PC/ç§»åŠ¨ç«¯ GUI
- **è§†è§‰ç¼–ç å¢å¼º**: ç”Ÿæˆ Draw.io/HTML/CSS/JS
- **"è¯†åˆ«ä¸€åˆ‡"**: åäººã€åŠ¨æ¼«ã€äº§å“ã€åœ°æ ‡ã€åŠ¨æ¤ç‰©

#### ğŸ“¹ é«˜çº§è§†é¢‘ç†è§£
- **åŸç”Ÿ 256K ä¸Šä¸‹æ–‡**, å¯æ‰©å±•è‡³ 1M
- **ç§’çº§è§†é¢‘ç´¢å¼•**: å¤„ç†å°æ—¶çº§è§†é¢‘
- **Text-Timestamp Alignment**: ç²¾ç¡®æ—¶é—´æˆ³å®šä½

#### ğŸ—ï¸ æ¶æ„å‡çº§
- **Interleaved-MRoPE**: å¤šç»´ä½ç½®ç¼–ç ï¼ˆæ—¶é—´ã€å®½åº¦ã€é«˜åº¦ï¼‰
- **DeepStack**: å¤šå±‚ ViT ç‰¹å¾èåˆï¼ˆç¬¬ 8ã€16ã€24 å±‚ï¼‰
- **32 è¯­è¨€ OCR**: æ”¯æŒä½å…‰ã€æ¨¡ç³Šã€å€¾æ–œåœºæ™¯



## ğŸ“ è®¡ç®—æ–¹æ³•è¯´æ˜

### å‚æ•°é‡è®¡ç®—å…¬å¼

#### 1ï¸âƒ£ **åŸºç¡€ç»„ä»¶å‚æ•°ï¼ˆæ¯å±‚ï¼‰**

```python
# Attention å‚æ•°ï¼ˆGQA - Grouped Query Attentionï¼‰
Attention = 4 Ã— hidden_sizeÂ² Ã— (1 + num_kv_heads / num_q_heads)
          = 4 Ã— 4096Â² Ã— (1 + 4/64)
          = 71,303,168 â‰ˆ 0.071B

# Router å‚æ•°ï¼ˆè·¯ç”±åˆ°ä¸“å®¶ï¼‰
Router = hidden_size Ã— num_experts
       = 4096 Ã— 512 (Intern-S1-Pro) æˆ– 4096 Ã— 128 (Qwen3-VL)
       = 2.10M (Intern) æˆ– 0.52M (Qwen)

# LayerNorm å‚æ•°ï¼ˆ2ä¸ªï¼špre-attn + post-attnï¼‰
LayerNorm = 2 Ã— hidden_size
          = 2 Ã— 4096
          = 8,192
```

#### 2ï¸âƒ£ **MoE ä¸“å®¶å‚æ•°ï¼ˆæ¯å±‚ï¼‰**

```python
# å•ä¸ªä¸“å®¶å‚æ•°ï¼ˆä½¿ç”¨ SwiGLUï¼‰
Expert_params = 3 Ã— hidden_size Ã— moe_intermediate_size
              = 3 Ã— 4096 Ã— 1536
              = 18,874,368 â‰ˆ 18.87M

# æ‰€æœ‰ä¸“å®¶æ€»å‚æ•°ï¼ˆæ¯å±‚ï¼‰
MoE_total = num_experts Ã— Expert_params
          = 512 Ã— 18.87M (Intern) æˆ– 128 Ã— 18.87M (Qwen)
          = 9.66B (Intern) æˆ– 2.42B (Qwen)

# æ¿€æ´»çš„ä¸“å®¶å‚æ•°ï¼ˆæ¯å±‚ï¼Œæ¨ç†æ—¶ï¼‰
MoE_activated = num_experts_per_tok Ã— Expert_params
              = 8 Ã— 18.87M
              = 150,994,944 â‰ˆ 0.151B
```

#### 3ï¸âƒ£ **æ€»å‚æ•°é‡ï¼ˆå®Œæ•´æ¨¡å‹ï¼‰**

```python
# Embedding å±‚
Embedding = vocab_size Ã— hidden_size
          = 155,008 Ã— 4096 (Intern) æˆ– 151,936 Ã— 4096 (Qwen)
          = 0.635B (Intern) æˆ– 0.622B (Qwen)

# æ‰€æœ‰ Transformer å±‚ï¼ˆåŒ…å«æ‰€æœ‰ä¸“å®¶ï¼‰
All_layers_total = num_layers Ã— (Attention + Router + LayerNorm + MoE_total)
                 = 94 Ã— (0.071B + 2.10M + 8,192 + 9.66B)    [Intern]
                 = 94 Ã— 9.737B
                 = 915.29B

# LM Headï¼ˆè¾“å‡ºå±‚ï¼‰
LM_head = hidden_size Ã— vocab_size
        = 0.635B (Intern) æˆ– 0.622B (Qwen)

# ã€æ€»å‚æ•°é‡ã€‘
Total = Embedding + All_layers_total + LM_head
      = 0.635B + 915.29B + 0.635B
      = 916.56B â‰ˆ 0.92T (Intern-S1-Pro)

      æˆ–

      = 0.622B + 233.85B + 0.622B
      = 235.09B (Qwen3-VL)
```

#### 4ï¸âƒ£ **æ¿€æ´»å‚æ•°é‡ï¼ˆæ¨ç†æ—¶ï¼‰**

```python
# æ‰€æœ‰å±‚çš„æ¿€æ´»å‚æ•°ï¼ˆä»…æ¿€æ´»éƒ¨åˆ†ä¸“å®¶ï¼‰
All_layers_activated = num_layers Ã— (Attention + Router + LayerNorm + MoE_activated)
                     = 94 Ã— (0.071B + 2.10M + 8,192 + 0.151B)
                     = 94 Ã— 0.224B
                     = 21.09B

# ã€æ¨ç†æ—¶æ¿€æ´»å‚æ•°ã€‘
Activated_total = Embedding + All_layers_activated + LM_head
                = 0.635B + 21.09B + 0.635B
                = 22.36B (Intern-S1-Pro)

                æˆ–

                = 0.622B + 20.94B + 0.622B
                = 22.19B (Qwen3-VL)
```

### ğŸ“Š è®¡ç®—ç»“æœå¯¹æ¯”

| æŒ‡æ ‡ | Intern-S1-Pro | Qwen3-VL | è¯´æ˜ |
|------|--------------|----------|------|
| **æ€»å‚æ•°** | 0.92T | 235.09B | Intern æ˜¯ Qwen çš„ 3.9 å€ |
| **æ¿€æ´»å‚æ•°** | 22.36B | 22.19B | å‡ ä¹ç›¸åŒï¼ˆå·®å¼‚ < 1%ï¼‰|
| **æ¿€æ´»ç‡** | 2.44% | 9.44% | Intern ä¸“å®¶æ›´å¤šï¼Œæ¿€æ´»ç‡æ›´ä½ |
| **ä¸“å®¶æ•°** | 512 | 128 | 4 å€å·®å¼‚ |
| **æ¯æ¬¡æ¿€æ´»** | 8 ä¸“å®¶ | 8 ä¸“å®¶ | ç›¸åŒ |


- ä¸“å®¶æ€»æ•°   â†’  å†³å®šæ€»å‚æ•°é‡ï¼ˆ920B vs 235Bï¼‰â†’ å½±å“éƒ¨ç½²æˆæœ¬
- æ¿€æ´»ä¸“å®¶æ•° â†’  å†³å®šæ¿€æ´»å‚æ•°é‡ï¼ˆ22.36B vs 22.19Bï¼‰â†’ å½±å“æ¨ç†æˆæœ¬
- Intern-S1-Proï¼šç”¨ 4 å€çš„éƒ¨ç½²æˆæœ¬ï¼Œæ¢å–æ›´å¼ºçš„ç§‘å­¦èƒ½åŠ›



## ğŸ“š å‚è€ƒèµ„æ–™

### Intern-S1-Pro
- ğŸ¤— [Hugging Face](https://huggingface.co/internlm/Intern-S1-Pro)
- ğŸ“„ [æŠ€æœ¯æŠ¥å‘Š](https://arxiv.org/abs/2508.15763)
- ğŸ  [å®˜æ–¹ä»“åº“](https://github.com/InternLM/Intern-S1)

### Qwen3-VL
- ğŸ¤— [Hugging Face](https://huggingface.co/Qwen/Qwen3-VL-235B-A22B-Instruct)
- ğŸ“„ [æŠ€æœ¯æŠ¥å‘Š(Qwen3)](https://arxiv.org/abs/2505.09388)
- ğŸ“„ [æŠ€æœ¯æŠ¥å‘Š(Qwen3-VL)](https://arxiv.org/abs/2511.21631)



## ğŸ“„ è®¸å¯è¯

- Intern-S1-Pro: Apache 2.0
- Qwen3-VL: Apache 2.0
